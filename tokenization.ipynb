{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ee9d898-1a61-4b3f-a513-690ed7f3b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f0fd4e7-34a3-4c83-9234-0212596468ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "textA = \"Tokenization is one of the basics in NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a29a8abc-7159-4d5b-83a8-854d2761461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is one of the basics in NLP.\n"
     ]
    }
   ],
   "source": [
    "print(textA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08688c86-321b-477c-87d3-38698b4e0e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization', 'is', 'one', 'of', 'the', 'basics', 'in', 'NLP.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textA.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf539c28-de87-44bc-bf73-f0523ca42fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textA.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afc803ef-dec1-4dda-a5cd-d9e2fd5dfc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization', 'is', 'one', 'of', 'the', 'basics', 'in', 'NLP', '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(textA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b4c3010-e072-4e0f-9e7a-81242f4e7714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(textA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6487dd55-46c6-46d4-b244-446663f45d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "textB = \"Tokenization breaks text into meaningful units (tokens). Tokens can be words, characters, or subwords.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53c8bdac-d5cb-4581-b1ce-9d68af132f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization breaks text into meaningful units (tokens).',\n",
       " 'Tokens can be words, characters, or subwords.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(textB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bacce664-e5e6-4fcb-8c42-e3db267fbb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.sent_tokenize(textB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
